{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monish/repos/train/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, models\n",
    "from timm import create_model\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as nprom torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToPILImage\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                                            | 0/72 [00:00<?, ?it/s]/home/monish/repos/train/venv/lib64/python3.11/site-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:310.)\n",
      "  return F.linear(input, self.weight, self.bias)\n",
      "Epoch 1/10: 100%|███████████████████████████████████████████████████| 72/72 [01:38<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.0790, Validation Accuracy: 54.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|███████████████████████████████████████████████████| 72/72 [01:25<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 1.2032, Validation Accuracy: 64.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|███████████████████████████████████████████████████| 72/72 [01:13<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.8391, Validation Accuracy: 70.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|███████████████████████████████████████████████████| 72/72 [01:17<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.7086, Validation Accuracy: 70.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|███████████████████████████████████████████████████| 72/72 [01:20<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.6568, Validation Accuracy: 69.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|███████████████████████████████████████████████████| 72/72 [01:14<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.6377, Validation Accuracy: 69.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|███████████████████████████████████████████████████| 72/72 [01:14<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.6289, Validation Accuracy: 69.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|███████████████████████████████████████████████████| 72/72 [01:14<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.6264, Validation Accuracy: 70.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|███████████████████████████████████████████████████| 72/72 [01:09<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.6185, Validation Accuracy: 70.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████████████████████████████████████████████| 72/72 [01:20<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.6191, Validation Accuracy: 69.58%\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'train/train'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    # transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for label, class_name in enumerate(os.listdir(root_dir)):\n",
    "            class_folder = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_folder):\n",
    "                for img_name in os.listdir(class_folder):\n",
    "                    img_path = os.path.join(class_folder, img_name)\n",
    "                    if img_path.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        self.image_paths.append(img_path)\n",
    "                        self.labels.append(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = read_image(img_path).float() \n",
    "        image = ToPILImage()(image) \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "dataset = ImageDataset(root_dir=train_dir, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = create_model('convnextv2_large', pretrained=True, num_classes=len(os.listdir(train_dir)))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def calculate_accuracy(loader, model, device):\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "epochs = 10 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", ncols=100):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    val_accuracy = calculate_accuracy(val_loader, model, device)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|███████████████████████████████████████████████████| 39/39 [00:14<00:00,  2.65it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dir = 'test/test' \n",
    "\n",
    "class_names = sorted(os.listdir(train_dir))\n",
    "\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        \n",
    "        for img_name in os.listdir(root_dir):\n",
    "            img_path = os.path.join(root_dir, img_name)\n",
    "            if img_path.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                self.image_paths.append(img_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = read_image(img_path).float()\n",
    "        image = ToPILImage()(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, img_path \n",
    "\n",
    "test_dataset = TestImageDataset(root_dir=test_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval() \n",
    "\n",
    "image_names = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, image_paths in tqdm(test_loader, desc=\"Predicting\", ncols=100):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for i in range(len(image_paths)):\n",
    "            image_name = Path(image_paths[i]).stem\n",
    "            image_names.append(image_name)\n",
    "            predicted_class_name = class_names[predicted[i].item()]\n",
    "            predicted_labels.append(predicted_class_name)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'name': image_names,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "df.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
